{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.3\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import the sklearn\n",
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_dia_f = load_diabetes(as_frame=True)\n",
    "l_dia_frame = l_dia_f.frame\n",
    "\n",
    "l_dia = load_diabetes()\n",
    "l_dia_data = l_dia.data\n",
    "l_dia_target = l_dia.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperation from train and test for digits\n",
    "X_dia_train, X_dia_test, y_dia_train, y_dia_test = train_test_split(l_dia_data, \n",
    "                                                    l_dia_target,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "X_dia_train = np.array(X_dia_train)\n",
    "X_dia_test = np.array(X_dia_test)\n",
    "y_dia_train = np.array(y_dia_train)\n",
    "y_dia_test = np.array(y_dia_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[159.51090192 189.00141284 145.09225849 302.14247511 138.28513671\n",
      " 130.86976779 226.30845916 196.26207086  98.95395246 117.1661962\n",
      "  97.13546024 129.27940657  65.76486314 239.46642902 146.82476628\n",
      " 157.00753999 247.30102089 265.46512135 187.12787401 217.61098821\n",
      " 167.33892696 104.50228674 111.06602394 198.54146975 143.79577382\n",
      " 184.34638842 177.38804173 180.03776483  64.92614743 149.01713571\n",
      " 198.80533693  94.42181404 144.75640008 199.32917697 210.17855876\n",
      " 146.55255018 153.56423322 151.96071504 213.52837288  75.57871903\n",
      " 140.24116601 147.68929402 147.60746754 211.24737648 182.7907714\n",
      "  64.96927716 101.79739972 140.66368502  84.06505254 122.48661462\n",
      " 152.5895225   50.92697939 122.16723959 142.95263486 133.81969415\n",
      " 127.19676859 130.76542883 188.08761451 105.20159293  97.12683729\n",
      " 176.65942347 172.3085754  133.0740562  150.04324861 136.12787094\n",
      " 172.3317631  151.34465575 153.77914896  76.66375822 119.69420631\n",
      " 147.50739368 196.71786709 297.40808911 124.3221     110.28187158\n",
      " 140.42177535 194.26303164 167.69418611 178.9268138  187.3042817\n",
      " 114.68847156  94.45093895  64.772471    92.32734018 142.1126576\n",
      " 132.50571384  81.78945123  85.45009266 127.17600706 190.73748351\n",
      " 169.3247409  202.01096601  62.44676109  83.8891146   43.17986727\n",
      " 211.24276378 256.24066556 196.29165886  65.13512253 128.13727575\n",
      " 191.77501727 106.28218753 269.07873511  81.37977644 170.99807275\n",
      "  61.46740314 122.82879696 160.34599156 196.41401691 173.89951243\n",
      " 171.04086639 196.00645769 226.66512581 251.43371106 206.00366349\n",
      " 141.59849731 144.44377275 136.83646006 104.94833379 134.18702761\n",
      " 161.7922907   87.7854396  128.99956151 151.32273728 165.66025643\n",
      " 209.14611746 176.04026034 135.61033439 183.1562544  102.3889581\n",
      " 155.22123524 147.89750424 229.95515058]\n",
      "[219.  70. 202. 230. 111.  84. 242. 272.  94.  96.  94. 252.  99. 297.\n",
      " 135.  67. 295. 264. 170. 275. 310.  64. 128. 232. 129. 118. 263.  77.\n",
      "  48. 107. 140. 113.  90. 164. 180. 233.  42.  84. 172.  63.  48. 108.\n",
      " 156. 168.  90.  52. 200.  87.  90. 258. 136. 158.  69.  72. 171.  95.\n",
      "  72. 151. 168.  60. 122.  52. 187. 102. 214. 248. 181. 110. 140. 202.\n",
      " 101. 222. 281.  61.  89.  91. 186. 220. 237. 233.  68. 190.  96.  72.\n",
      " 153.  98.  37.  63. 184. 144. 150. 280. 125.  59.  65. 281. 277. 167.\n",
      "  90.  72. 178.  88. 270. 101. 197.  97.  53.  71. 262.  52. 102. 166.\n",
      " 189. 173. 220. 206.  97.  60.  61. 242. 121. 128. 104. 265. 132. 283.\n",
      " 174. 129. 257. 137.  63.  93. 232.]\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Union, Optional\n",
    "import pandas as pd\n",
    "\n",
    "class CustomGD:\n",
    "    def __init__(self, l_rate: Optional[Union[float, np.float64]], epoch: Optional[int], *, rms: bool=True):\n",
    "        self.l_rate = None\n",
    "        self.epoch = None\n",
    "\n",
    "        if type(l_rate) == None:\n",
    "            self.l_rate = 0.1\n",
    "        else:\n",
    "            self.l_rate = l_rate\n",
    "\n",
    "        if type(epoch) == None:\n",
    "            self.epoch = 100\n",
    "        else:\n",
    "            self.epoch = epoch\n",
    "\n",
    "        # for manual definition with model function\n",
    "        m_func = None\n",
    "\n",
    "    def _first_order_func(self, x: Union[float, np.float64, np.ndarray], w: Union[float, np.float64, np.ndarray], b: Union[float, np.float64]) -> Union[float, np.float64, np.ndarray]:\n",
    "        return np.dot(w, x) + b\n",
    "\n",
    "    def _gradient_multi(self, x: np.ndarray, w: Union[float, np.float64, np.ndarray], b: Union[float, np.float64], y: np.ndarray) -> Tuple[Union[float, np.float64, np.ndarray], Union[float, np.float64]]:\n",
    "\n",
    "        N = 10\n",
    "        \n",
    "        w_grad = np.zeros(shape=(1, N))\n",
    "        b_grad = 0\n",
    "\n",
    "        # Calculate for Loss\n",
    "        for i in range(N):\n",
    "            pred = np.dot(w, x[i])  + b\n",
    "            w_grad = w_grad+(-2) * x[i] * (y[i] - (pred))\n",
    "            b_grad = b_grad+(-2) * (y[i] - (pred))\n",
    "            \n",
    "            # pred = self._first_order_func(x=x[i], w=w, b=b)\n",
    "            # w_grad = w_grad + ((-2) * x[i] * (y[i] - pred))\n",
    "            # b_grad = b_grad + ((-2) * (y[i] - pred))\n",
    "\n",
    "        w = w - (self.l_rate * (w_grad / N))\n",
    "        b = b - (self.l_rate * (b_grad / N))\n",
    "        \n",
    "        return w, b\n",
    "\n",
    "    def get_model(self, m_func: Callable) -> None:\n",
    "        self.m_func = m_func\n",
    "\n",
    "    def mse_multi(self, target: Union[np.float64, np.ndarray], pred: Union[np.float64, np.ndarray], *,rms: bool=False) -> Union[float, np.float64, np.ndarray]:\n",
    "\n",
    "        return np.square(np.subtract(target, pred)).mean()\n",
    "\n",
    "    def learn(self, X_train: np.ndarray, y_train: np.ndarray, *, random: bool=False) -> None:\n",
    "        # initialize w var in y train size\n",
    "        w = np.zeros(shape=(1, X_train.shape[1]))\n",
    "        b = 0\n",
    "    \n",
    "        # Learning in as much as number of n\n",
    "        for i in range(self.epoch):\n",
    "            w, b = self._gradient_multi(x=X_train, w=w, b=b, y=y_train)\n",
    "            #print(w)\n",
    "        return w, b\n",
    "\n",
    "    def predict(\n",
    "        self, x_test: np.ndarray, w: Union[float, np.ndarray], b: float\n",
    "    ) -> list:\n",
    "        l_pred = [self._first_order_func(x=x, w=w, b=b).item() for x in x_test]\n",
    "        #print(l_pred)\n",
    "\n",
    "        return np.array(l_pred)\n",
    "\n",
    "\n",
    "cgd = CustomGD(0.1, 2000, rms=False)\n",
    "rw, rb = cgd.learn(X_dia_train, y_dia_train)\n",
    "y_dia_pred = cgd.predict(X_dia_test, rw, rb)\n",
    "mean_res = cgd.mse_multi(y_dia_test, y_dia_pred)\n",
    "print(y_dia_pred)\n",
    "print(y_dia_test)\n",
    "\n",
    "if mean_res < 3000:\n",
    "    print(f\"MSE value : {mean_res}\")\n",
    "    plt.scatter(y_dia_test, y_dia_pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000]\n",
      "learning rate: 0.1, epoch : 100, MSE : 5910.876999598644\n",
      "learning rate: 0.1, epoch : 500, MSE : 3777.041609399888\n",
      "learning rate: 0.1, epoch : 1000, MSE : 3375.691923888584\n",
      "learning rate: 0.1, epoch : 1500, MSE : 3389.6459663600513\n",
      "learning rate: 0.1, epoch : 2000, MSE : 3472.4210183837545\n",
      "learning rate: 0.1, epoch : 2500, MSE : 3564.2840786747524\n",
      "learning rate: 0.1, epoch : 3000, MSE : 3650.4691257964937\n",
      "learning rate: 0.1, epoch : 3500, MSE : 3726.932629080443\n",
      "learning rate: 0.1, epoch : 4000, MSE : 3793.165165184298\n",
      "learning rate: 0.05, epoch : 100, MSE : 6529.923084978028\n",
      "learning rate: 0.05, epoch : 500, MSE : 4681.274506751401\n",
      "learning rate: 0.05, epoch : 1000, MSE : 3777.566897400696\n",
      "learning rate: 0.05, epoch : 1500, MSE : 3468.831886817952\n",
      "learning rate: 0.05, epoch : 2000, MSE : 3375.8139995678653\n",
      "learning rate: 0.05, epoch : 2500, MSE : 3365.7366023816226\n",
      "learning rate: 0.05, epoch : 3000, MSE : 3389.648824733294\n",
      "learning rate: 0.05, epoch : 3500, MSE : 3428.0532240698394\n",
      "learning rate: 0.05, epoch : 4000, MSE : 3472.3880245576574\n",
      "learning rate: 0.025, epoch : 100, MSE : 6799.9189537445845\n",
      "learning rate: 0.025, epoch : 500, MSE : 5648.590958853724\n",
      "learning rate: 0.025, epoch : 1000, MSE : 4681.65905375607\n",
      "learning rate: 0.025, epoch : 1500, MSE : 4112.731609569096\n",
      "learning rate: 0.025, epoch : 2000, MSE : 3777.829488664389\n",
      "learning rate: 0.025, epoch : 2500, MSE : 3581.7599698977356\n",
      "learning rate: 0.025, epoch : 3000, MSE : 3468.9677022883557\n",
      "learning rate: 0.025, epoch : 3500, MSE : 3406.797981974984\n",
      "learning rate: 0.025, epoch : 4000, MSE : 3375.875076486345\n",
      "learning rate: 0.0125, epoch : 100, MSE : 6003.3861553248835\n",
      "learning rate: 0.0125, epoch : 500, MSE : 6363.253707713602\n",
      "learning rate: 0.0125, epoch : 1000, MSE : 5648.75630440533\n",
      "learning rate: 0.0125, epoch : 1500, MSE : 5101.444558634097\n",
      "learning rate: 0.0125, epoch : 2000, MSE : 4681.851265746041\n",
      "learning rate: 0.0125, epoch : 2500, MSE : 4359.9487552546\n",
      "learning rate: 0.0125, epoch : 3000, MSE : 4112.8997955087925\n",
      "learning rate: 0.0125, epoch : 3500, MSE : 3923.3198094687073\n",
      "learning rate: 0.0125, epoch : 4000, MSE : 3777.960771106615\n",
      "learning rate: 0.00625, epoch : 100, MSE : 5372.899438838266\n",
      "learning rate: 0.00625, epoch : 500, MSE : 6769.93769596714\n",
      "learning rate: 0.00625, epoch : 1000, MSE : 6363.304124051628\n",
      "learning rate: 0.00625, epoch : 1500, MSE : 5982.277939617168\n",
      "learning rate: 0.00625, epoch : 2000, MSE : 5648.838959256967\n",
      "learning rate: 0.00625, epoch : 2500, MSE : 5357.009977552945\n",
      "learning rate: 0.00625, epoch : 3000, MSE : 5101.539043829027\n",
      "learning rate: 0.00625, epoch : 3500, MSE : 4877.84841135987\n",
      "learning rate: 0.00625, epoch : 4000, MSE : 4681.947356369882\n",
      "learning rate: 0.003125, epoch : 100, MSE : 8711.345968783\n",
      "learning rate: 0.003125, epoch : 500, MSE : 6394.065358077638\n",
      "learning rate: 0.003125, epoch : 1000, MSE : 6769.370290265642\n",
      "learning rate: 0.003125, epoch : 1500, MSE : 6572.660589507667\n",
      "learning rate: 0.003125, epoch : 2000, MSE : 6363.329235204437\n",
      "learning rate: 0.003125, epoch : 2500, MSE : 6166.480883543658\n",
      "learning rate: 0.003125, epoch : 3000, MSE : 5982.313446852125\n",
      "learning rate: 0.003125, epoch : 3500, MSE : 5810.0386947858915\n",
      "learning rate: 0.003125, epoch : 4000, MSE : 5648.880282202914\n",
      "learning rate: 0.0015625, epoch : 100, MSE : 14603.139536787194\n",
      "learning rate: 0.0015625, epoch : 500, MSE : 5261.481050919868\n",
      "learning rate: 0.0015625, epoch : 1000, MSE : 6391.2023457312935\n",
      "learning rate: 0.0015625, epoch : 1500, MSE : 6773.473257754637\n",
      "learning rate: 0.0015625, epoch : 2000, MSE : 6769.084106252469\n",
      "learning rate: 0.0015625, epoch : 2500, MSE : 6678.429747916688\n",
      "learning rate: 0.0015625, epoch : 3000, MSE : 6572.652909546269\n",
      "learning rate: 0.0015625, epoch : 3500, MSE : 6466.6343915948355\n",
      "learning rate: 0.0015625, epoch : 4000, MSE : 6363.34176590829\n",
      "learning rate: 0.00078125, epoch : 100, MSE : 19793.76760857441\n",
      "learning rate: 0.00078125, epoch : 500, MSE : 7198.110327566979\n",
      "learning rate: 0.00078125, epoch : 1000, MSE : 5261.109820055192\n",
      "learning rate: 0.00078125, epoch : 1500, MSE : 5847.10111641634\n",
      "learning rate: 0.00078125, epoch : 2000, MSE : 6389.771298380728\n",
      "learning rate: 0.00078125, epoch : 2500, MSE : 6666.830759303749\n",
      "learning rate: 0.00078125, epoch : 3000, MSE : 6772.953154346631\n",
      "learning rate: 0.00078125, epoch : 3500, MSE : 6791.587752411471\n",
      "learning rate: 0.00078125, epoch : 4000, MSE : 6768.9403937374245\n",
      "learning rate: 0.000390625, epoch : 100, MSE : 23210.18953058137\n",
      "learning rate: 0.000390625, epoch : 500, MSE : 12676.988975272983\n",
      "learning rate: 0.000390625, epoch : 1000, MSE : 7200.4829977539775\n",
      "learning rate: 0.000390625, epoch : 1500, MSE : 5502.612835874334\n",
      "learning rate: 0.000390625, epoch : 2000, MSE : 5260.926158617476\n",
      "learning rate: 0.000390625, epoch : 2500, MSE : 5503.602789121814\n",
      "learning rate: 0.000390625, epoch : 3000, MSE : 5846.249245103858\n",
      "learning rate: 0.000390625, epoch : 3500, MSE : 6153.149627918215\n",
      "learning rate: 0.000390625, epoch : 4000, MSE : 6389.055891093211\n"
     ]
    }
   ],
   "source": [
    "def run(random: bool=False) -> None:\n",
    "    epoch_list = []\n",
    "    l_rate = 0.1\n",
    "\n",
    "    epoch_list.append(100)\n",
    "    epoch_list.append(500)\n",
    "\n",
    "    for i in range(len(epoch_list) -1, 10 - len(epoch_list)):\n",
    "        epoch_list.append(epoch_list[i] + 500)\n",
    "    print(epoch_list)\n",
    "\n",
    "    t_l_rate = l_rate\n",
    "    for _ in range(len(epoch_list)):\n",
    "        for i in epoch_list:\n",
    "            cgd = CustomGD(t_l_rate, i)\n",
    "            rw, rb = cgd.learn(X_dia_train, y_dia_train, random=True)\n",
    "            y_dia_pred = cgd.predict(X_dia_test, rw, rb)\n",
    "            mean_res = cgd.mse_multi(y_dia_test, y_dia_pred)\n",
    "\n",
    "            print(f\"learning rate: {t_l_rate}, epoch : {i}, MSE : {mean_res}\")\n",
    "\n",
    "            if mean_res < 3000:\n",
    "                print(f\"MSE value : {mean_res}\")\n",
    "                plt.scatter(y_dia_test, y_dia_pred)\n",
    "                plt.show()\n",
    "            \n",
    "        t_l_rate /= 2\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20b65602904a6ff3fc15d4434a7c8a93588f98c76b1baa36944f5d2c44ba5b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
